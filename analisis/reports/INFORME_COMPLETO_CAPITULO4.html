<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Results Report: Comparative Evaluation of Manual Tests vs. AI-Generated Tests</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Times New Roman', Times, serif;
            line-height: 1.6;
            color: #222;
            background: #ffffff;
            padding: 0;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background: white;
            padding: 50px 40px;
        }
        
        .header {
            text-align: center;
            margin-bottom: 40px;
            border-bottom: 2px solid #333;
            padding-bottom: 20px;
        }
        
        .header h1 {
            font-size: 1.8em;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 1px;
            font-weight: bold;
        }
        
        .header p {
            font-size: 1em;
            margin: 8px 0;
            font-style: italic;
        }
        
        .content {
            line-height: 1.8;
        }
        
        h1 {
            font-size: 1.5em;
            margin-top: 35px;
            margin-bottom: 15px;
            text-transform: uppercase;
            font-weight: bold;
        }
        
        h2 {
            font-size: 1.2em;
            margin-top: 25px;
            margin-bottom: 12px;
            font-weight: bold;
            color: #333;
        }
        
        p {
            margin-bottom: 12px;
            text-align: justify;
        }
        
        ul, ol {
            margin-left: 30px;
            margin-bottom: 12px;
        }
        
        li {
            margin-bottom: 6px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            border: 1px solid #999;
        }
        
        th, td {
            border: 1px solid #999;
            padding: 10px 12px;
            text-align: left;
        }
        
        th {
            background-color: #e8e8e8;
            font-weight: bold;
        }
        
        tr:nth-child(even) {
            background-color: #f8f8f8;
        }
        
        .figure {
            text-align: center;
            margin: 30px 0;
        }
        
        .figure img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ccc;
            margin-bottom: 10px;
        }
        
        .figure-caption {
            font-style: italic;
            color: #555;
            font-size: 0.9em;
        }
        
        .highlight {
            background-color: #f5f5f5;
            padding: 15px;
            margin: 15px 0;
            border-left: 3px solid #333;
        }
        
        .footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 2px solid #333;
            text-align: center;
            font-size: 0.9em;
            color: #666;
        }
        
        @media print {
            body { background: white; }
            .container { padding: 20px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Comparative Evaluation of Manual Tests vs. AI-Generated Tests</h1>
            <p>Experimental Results Report</p>
            <p style="font-size: 0.9em; margin-top: 15px;">System Under Test: Spring PetClinic | Period: May-December 2025</p>
        </div>
        
        <div class="content">
            
            <h1>1. INTRODUCTION</h1>
            
            <p>This report presents the results of an empirical comparative study conducted between May and December 2025, aimed at evaluating the effectiveness of automatically generated test cases using generative artificial intelligence compared to those designed manually in Java and Spring Boot environments. The Spring PetClinic project was used as the system under test (SUT), obtaining 2,480 observations distributed across 40 experimental iterations, combining objective quantitative metrics with empirical observations on functional understanding.</p>
            
            <h1>2. METHODOLOGY</h1>
            
            <h2>2.1 Experimental Design</h2>
            
            <p>The study adopted a mixed empirical-comparative design approach, evaluating two test generation approaches:</p>
            <ul>
                <li><strong>Manual Tests:</strong> Designed by the researcher using JUnit 5 and MockMvc</li>
                <li><strong>AI Tests:</strong> Generated using ChatGPT (GPT-5) and Diffblue Cover CLI (Teams Edition)</li>
            </ul>
            
            <h2>2.2 Metrics and Data Collection</h2>
            
            <p>Data collection was performed using professional standardized tools:</p>
            <ul>
                <li><strong>JaCoCo 0.8.12:</strong> Measurement of statement coverage and branch coverage</li>
                <li><strong>PITest 1.16.0:</strong> Calculation of mutation score</li>
                <li><strong>Maven Surefire 3.2.5:</strong> Recording of execution times</li>
                <li><strong>Python 3.12.6:</strong> Statistical analysis (pandas, scipy, matplotlib)</li>
            </ul>
            
            <p>40 iterations per test type were executed, preceded by 3 warm-up cycles to stabilize the JVM, generating a dataset of 2,480 observations (1,600 manual + 880 AI-generated).</p>
            
            <h2>2.3 Statistical Analysis</h2>
            
            <p>The data was subjected to verification of statistical assumptions using the Shapiro-Wilk test (normality) and Levene test (homogeneity of variances). Since the raw data violated the normality assumption, the non-parametric Mann-Whitney U test was used as the main analysis, with t-Student as secondary validation on aggregated data (N=12).</p>
            
            <h1>3. RESULTS</h1>
            
            <h2>3.1 Experiment Execution and Dataset Structure</h2>
            
            <p>The experimental execution was carried out through a fully automated process, designed to ensure consistency and eliminate any human intervention during data collection. 40 iterations were performed for each of the 12 test classes considered in the study (6 manually designed and 6 generated using artificial intelligence tools), obtaining a total of 2,480 raw records. The complete execution process required 6.18 hours, considering valid iterations and preliminary executions used to stabilize the JVM.</p>
            
            <p><strong>Table 1:</strong> Components and details of the experimental execution.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Quantity</th>
                        <th>Detail</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Total test classes</strong></td>
                        <td>12</td>
                        <td>6 Manual + 6 AI-Generated</td>
                    </tr>
                    <tr>
                        <td><strong>Iterations per class</strong></td>
                        <td>40</td>
                        <td>Includes warm-up (3 initial iterations)</td>
                    </tr>
                    <tr>
                        <td><strong>Records per iteration</strong></td>
                        <td>2</td>
                        <td>Compilation and execution</td>
                    </tr>
                    <tr>
                        <td><strong>Total raw records</strong></td>
                        <td>2,480</td>
                        <td>1,600 Manual + 880 AI</td>
                    </tr>
                    <tr>
                        <td><strong>Metrics per record</strong></td>
                        <td>4</td>
                        <td>instruction%, branch%, mutation%, time(s)</td>
                    </tr>
                    <tr>
                        <td><strong>Total execution time</strong></td>
                        <td>6.18 hours</td>
                        <td>371 continuous minutes</td>
                    </tr>
                </tbody>
            </table>
            
            <h2>3.2 Dataset Preparation</h2>
            
            <p>The raw records were consolidated into a single master file using automated Python scripts. Two aggregation levels were defined: the complete level (N=2,480) preserved all individual records, and the aggregated level (N=12) was obtained by averaging the 40 iterations per test class.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Analysis Level</th>
                        <th>Total Records</th>
                        <th>Manual</th>
                        <th>AI</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>N = 2,480</strong></td>
                        <td>2,480</td>
                        <td>1,600</td>
                        <td>880</td>
                        <td>Individual observations per iteration</td>
                    </tr>
                    <tr>
                        <td><strong>N = 12</strong></td>
                        <td>12</td>
                        <td>6</td>
                        <td>6</td>
                        <td>Independent averages per test class</td>
                    </tr>
                </tbody>
            </table>
            
            <h2>3.3 Descriptive Statistics (Aggregated Level, N=12)</h2>
            
            <p><strong>Table 2:</strong> Descriptive statistics with averages of 40 iterations per test class.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Group</th>
                        <th>N</th>
                        <th>Mean</th>
                        <th>Median</th>
                        <th>Std. Dev.</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td rowspan="2"><strong>Instruction Coverage (%)</strong></td>
                        <td>Manual</td>
                        <td>6</td>
                        <td>18.25%</td>
                        <td>15.95%</td>
                        <td>12.50%</td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>6</td>
                        <td>17.67%</td>
                        <td>16.13%</td>
                        <td>11.36%</td>
                    </tr>
                    <tr>
                        <td rowspan="2"><strong>Branch Coverage (%)</strong></td>
                        <td>Manual</td>
                        <td>6</td>
                        <td>14.58%</td>
                        <td>11.88%</td>
                        <td>12.62%</td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>6</td>
                        <td>12.05%</td>
                        <td>9.89%</td>
                        <td>9.49%</td>
                    </tr>
                    <tr>
                        <td rowspan="2"><strong>Mutation Score (%)</strong></td>
                        <td>Manual</td>
                        <td>6</td>
                        <td>18.52%</td>
                        <td>15.28%</td>
                        <td>17.71%</td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>6</td>
                        <td>14.76%</td>
                        <td>15.11%</td>
                        <td>11.61%</td>
                    </tr>
                    <tr>
                        <td rowspan="2"><strong>Execution Time (s)</strong></td>
                        <td>Manual</td>
                        <td>6</td>
                        <td>0.082s</td>
                        <td>0.071s</td>
                        <td>0.069s</td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>6</td>
                        <td>0.194s</td>
                        <td>0.144s</td>
                        <td>0.193s</td>
                    </tr>
                </tbody>
            </table>
            
            <h2>3.4 Descriptive Statistics (Complete Level, N=2,480)</h2>
            
            <p><strong>Table 3:</strong> Individual records per iteration, representing the complete dataset.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Group</th>
                        <th>N</th>
                        <th>Mean</th>
                        <th>Median</th>
                        <th>Std. Dev.</th>
                        <th>Range</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td rowspan="2"><strong>Instruction Coverage (%)</strong></td>
                        <td>Manual</td>
                        <td>1,600</td>
                        <td>19.94%</td>
                        <td>21.85%</td>
                        <td>12.11%</td>
                        <td>5.03%-35.56%</td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>880</td>
                        <td>14.20%</td>
                        <td>10.69%</td>
                        <td>8.71%</td>
                        <td>0%-33%</td>
                    </tr>
                    <tr>
                        <td rowspan="2"><strong>Branch Coverage (%)</strong></td>
                        <td>Manual</td>
                        <td>1,600</td>
                        <td>17.69%</td>
                        <td>12.50%</td>
                        <td>12.39%</td>
                        <td>2.50%-37.50%</td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>880</td>
                        <td>13.50%</td>
                        <td>16.25%</td>
                        <td>6.75%</td>
                        <td>0%-28.75%</td>
                    </tr>
                    <tr>
                        <td rowspan="2"><strong>Mutation Score (%)</strong></td>
                        <td>Manual</td>
                        <td>1,600</td>
                        <td>22.92%</td>
                        <td>16.67%</td>
                        <td>17.19%</td>
                        <td>0%-50%</td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>880</td>
                        <td>16.63%</td>
                        <td>19.44%</td>
                        <td>8.16%</td>
                        <td>0%-33.33%</td>
                    </tr>
                    <tr>
                        <td rowspan="2"><strong>Execution Time (s)</strong></td>
                        <td>Manual</td>
                        <td>1,600</td>
                        <td>0.0794s</td>
                        <td>0.0120s</td>
                        <td>0.1775s</td>
                        <td>0-0.861s</td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>880</td>
                        <td>0.1140s</td>
                        <td>0.0030s</td>
                        <td>0.2318s</td>
                        <td>0-0.846s</td>
                    </tr>
                </tbody>
            </table>
            
            <h2>3.5 Verification of Statistical Assumptions</h2>
            
            <h3>3.5.1 Shapiro-Wilk Normality Test (N=2,480)</h3>
            
            <p><strong>Table 4:</strong> All metrics show p &lt; 0.05, indicating absence of normality.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Group</th>
                        <th>W</th>
                        <th>p-value</th>
                        <th>Normality</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td rowspan="2">Instruction Coverage</td>
                        <td>Manual</td>
                        <td>0.8160</td>
                        <td>2.34e-39</td>
                        <td><strong>No</strong></td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>0.7524</td>
                        <td>2.66e-34</td>
                        <td><strong>No</strong></td>
                    </tr>
                    <tr>
                        <td rowspan="2">Branch Coverage</td>
                        <td>Manual</td>
                        <td>0.8178</td>
                        <td>3.21e-39</td>
                        <td><strong>No</strong></td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>0.8697</td>
                        <td>1.96e-26</td>
                        <td><strong>No</strong></td>
                    </tr>
                    <tr>
                        <td rowspan="2">Mutation Score</td>
                        <td>Manual</td>
                        <td>0.8404</td>
                        <td>2.46e-37</td>
                        <td><strong>No</strong></td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>0.8313</td>
                        <td>1.82e-29</td>
                        <td><strong>No</strong></td>
                    </tr>
                    <tr>
                        <td rowspan="2">Execution Time</td>
                        <td>Manual</td>
                        <td>0.4797</td>
                        <td>8.07e-56</td>
                        <td><strong>No</strong></td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>0.5354</td>
                        <td>4.67e-43</td>
                        <td><strong>No</strong></td>
                    </tr>
                </tbody>
            </table>
            
            <h3>3.5.2 Shapiro-Wilk Normality Test (N=12)</h3>
            
            <p><strong>Table 5:</strong> All metrics show p &gt; 0.05, meeting the normality assumption.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Group</th>
                        <th>W</th>
                        <th>p-value</th>
                        <th>Normality</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td rowspan="2">Instruction Coverage</td>
                        <td>Manual</td>
                        <td>0.9087</td>
                        <td>0.4281</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>0.9038</td>
                        <td>0.3971</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                    <tr>
                        <td rowspan="2">Branch Coverage</td>
                        <td>Manual</td>
                        <td>0.8807</td>
                        <td>0.2722</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>0.9127</td>
                        <td>0.4541</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                    <tr>
                        <td rowspan="2">Mutation Score</td>
                        <td>Manual</td>
                        <td>0.9132</td>
                        <td>0.4581</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>0.9720</td>
                        <td>0.9056</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                    <tr>
                        <td rowspan="2">Execution Time</td>
                        <td>Manual</td>
                        <td>0.9296</td>
                        <td>0.5771</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                    <tr>
                        <td>AI</td>
                        <td>0.8507</td>
                        <td>0.1596</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                </tbody>
            </table>
            
            <h3>3.5.3 Levene's Test for Variance Homogeneity (N=12)</h3>
            
            <p><strong>Table 6:</strong> Evaluation of variance homogeneity between groups.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>F-statistic</th>
                        <th>p-value</th>
                        <th>Homogeneity</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Instruction Coverage</td>
                        <td>0.1006</td>
                        <td>0.7577</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                    <tr>
                        <td>Branch Coverage</td>
                        <td>0.1262</td>
                        <td>0.7298</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                    <tr>
                        <td>Mutation Score</td>
                        <td>0.3930</td>
                        <td>0.5448</td>
                        <td><strong>Yes</strong></td>
                    </tr>
                    <tr>
                        <td>Execution Time</td>
                        <td>6.3679</td>
                        <td>0.0302</td>
                        <td><strong>No</strong></td>
                    </tr>
                </tbody>
            </table>
            
            <h2>3.6 Parametric Tests (t-Student and Welch, N=12)</h2>
            
            <p><strong>Table 7:</strong> Parametric contrasts performed on aggregated data. Standard t-Student was applied for 3 metrics and Welch correction for time (variance heterogeneity detected by Levene).</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Manual Mean</th>
                        <th>AI Mean</th>
                        <th>Test Used</th>
                        <th>t-statistic</th>
                        <th>p-value</th>
                        <th>Significant</th>
                        <th>Cohen's d</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Instruction Coverage</td>
                        <td>18.25%</td>
                        <td>17.67%</td>
                        <td>t-Standard</td>
                        <td>0.0836</td>
                        <td>0.9350</td>
                        <td><strong>No</strong></td>
                        <td>0.0483</td>
                    </tr>
                    <tr>
                        <td>Branch Coverage</td>
                        <td>14.58%</td>
                        <td>12.05%</td>
                        <td>t-Standard</td>
                        <td>0.3935</td>
                        <td>0.7022</td>
                        <td><strong>No</strong></td>
                        <td>0.2272</td>
                    </tr>
                    <tr>
                        <td>Mutation Score</td>
                        <td>18.52%</td>
                        <td>14.76%</td>
                        <td>t-Standard</td>
                        <td>0.4352</td>
                        <td>0.6727</td>
                        <td><strong>No</strong></td>
                        <td>0.2512</td>
                    </tr>
                    <tr style="background-color: #fff0f0;">
                        <td>Execution Time</td>
                        <td>0.0823s</td>
                        <td>0.1938s</td>
                        <td>t-Welch</td>
                        <td>-1.3324</td>
                        <td>0.2293</td>
                        <td><strong>No</strong></td>
                        <td>-0.7693</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="highlight">
                <strong>Important Finding:</strong> The parametric analysis (N=12) does not detect significant differences in any metric. However, N=12 has low statistical power (~30-50%), limiting its detection capability. The main analysis should be Mann-Whitney U with N=2,480.
            </div>
            
            <h2>3.7 Non-Parametric Tests (Mann-Whitney U, N=2,480)</h2>
            
            <p><strong>Table 8:</strong> Main analysis with 2,480 observations. Mann-Whitney U was selected because raw data violated the normality assumption (Shapiro-Wilk rejected in all metrics).</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Manual Median</th>
                        <th>AI Median</th>
                        <th>U-statistic</th>
                        <th>p-value</th>
                        <th>Significance</th>
                        <th>Effect Size (r)</th>
                        <th>Conclusion</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Instruction Coverage</strong></td>
                        <td>21.85%</td>
                        <td>10.69%</td>
                        <td>827,440</td>
                        <td>3.20e-13</td>
                        <td>***</td>
                        <td>0.1453</td>
                        <td>Manual &gt; AI</td>
                    </tr>
                    <tr>
                        <td><strong>Branch Coverage</strong></td>
                        <td>12.50%</td>
                        <td>16.25%</td>
                        <td>808,720</td>
                        <td>5.73e-10</td>
                        <td>***</td>
                        <td>0.1232</td>
                        <td>AI &gt; Manual</td>
                    </tr>
                    <tr>
                        <td><strong>Mutation Score</strong></td>
                        <td>16.67%</td>
                        <td>19.44%</td>
                        <td>752,840</td>
                        <td>3.82e-03</td>
                        <td>**</td>
                        <td>0.0575</td>
                        <td>AI &gt; Manual</td>
                    </tr>
                    <tr>
                        <td><strong>Execution Time</strong></td>
                        <td>0.0120s</td>
                        <td>0.0030s</td>
                        <td>774,034</td>
                        <td>3.74e-05</td>
                        <td>***</td>
                        <td>0.0824</td>
                        <td>AI faster</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="highlight">
                <strong>Statistical Significance:</strong> *** p &lt; 0.001 (highly significant) | ** p &lt; 0.01 (significant). All metrics show p-values well below the 0.05 threshold, indicating statistically significant differences in the complete data.
            </div>
            
            <p>The non-parametric Mann-Whitney U test was applied to identify statistically significant differences. The results demonstrate significant differences in all evaluated metrics:</p>
            
            <p><strong>Table 10:</strong> Summary of Mann-Whitney U statistical results.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>p-value</th>
                        <th>Significance</th>
                        <th>Effect Size (r)</th>
                        <th>Conclusion</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Instruction Coverage</strong></td>
                        <td>3.20e-13</td>
                        <td>***</td>
                        <td>0.1453</td>
                        <td>Manual significantly > AI</td>
                    </tr>
                    <tr>
                        <td><strong>Branch Coverage</strong></td>
                        <td>5.73e-10</td>
                        <td>***</td>
                        <td>0.1232</td>
                        <td>AI significantly > Manual</td>
                    </tr>
                    <tr>
                        <td><strong>Mutation Score</strong></td>
                        <td>3.82e-03</td>
                        <td>**</td>
                        <td>0.0575</td>
                        <td>AI significantly > Manual</td>
                    </tr>
                    <tr>
                        <td><strong>Execution Time</strong></td>
                        <td>3.74e-05</td>
                        <td>***</td>
                        <td>0.0824</td>
                        <td>AI significantly faster</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="highlight">
                <strong>Note on Significance:</strong> *** indicates p &lt; 0.001 (highly significant), ** indicates p &lt; 0.01 (significant). All p-values are well below the 0.05 threshold.
            </div>
            
            <h2>3.9 Graphical Visualization of Results</h2>
            
            <p>The following figures present visual comparisons of metrics under parametric analysis (t-Student, N=12) and non-parametric (Mann-Whitney U, N=2,480). These visualizations complement the statistical findings reported in the previous tables.</p>
            
            <div class="figure">
                <img src="../figures/06_t_Student_2x2.png" alt="Figure 3.5 - Parametric t-Student Comparison (N=12)">
                <p class="figure-caption"><strong>Figure 3.5:</strong> Parametric t-Student Comparison (N=12). Box plots showing distribution, mean (red diamond) and dispersion of metrics for manual and AI tests. This visualization uses aggregated data per class and presents a purely descriptive purpose of the central data behavior.</p>
            </div>
            
            <div class="figure">
                <img src="../figures/07_Mann_Whitney_2x2.png" alt="Figure 3.6 - Non-Parametric Mann-Whitney U Comparison (N=2,480)">
                <p class="figure-caption"><strong>Figure 3.6:</strong> Non-Parametric Mann-Whitney U Distribution (N=2,480). Violin plots illustrating the complete shape and density of distributions for the main analysis with all individual observations. Wider areas represent greater value concentration. This is the visualization corresponding to the definitive statistical analysis.</p>
            </div>
            
            <h1>4. DETAILED DISCUSSION OF RESULTS</h1>
            
            <h2>4.1 Analysis of Instruction Coverage</h2>
            
            <p>Manual tests achieve significantly higher instruction coverage than AI-generated ones (Manual median: 21.85% vs AI: 10.69%, p = 3.20e-13). This difference of approximately 11 percentage points is statistically significant and has practical importance. The result indicates that the manual test design process, based on code inspection and human reasoning, allows for a more exhaustive exploration of individual instructions. Manual tests tend to cover not only main paths but also secondary paths and special cases.</p>
            
            <h2>4.2 Analysis of Branch Coverage</h2>
            
            <p>Contrary to instruction coverage, AI-generated tests achieve superior branch coverage compared to manual ones (AI median: 16.25% vs Manual: 12.50%, p = 5.73e-10). This inverse pattern suggests that AI, through bytecode analysis (especially Diffblue Cover), is more effective in exploring alternative paths and decision branches in the code. This can be attributed to automatic generation algorithms systematically testing multiple execution paths without the influence of human cognitive biases.</p>
            
            <h2>4.3 Analysis of Mutation Score</h2>
            
            <p>AI-generated tests present statistically superior mutation score (AI median: 19.44% vs Manual: 16.67%, p = 3.82e-03). Although the difference is statistically significant, the effect size is small (r = 0.0575), indicating that the mutant detection capability is comparable between both approaches. This result suggests that both testing strategies are reasonably effective for identifying intentional changes in code (mutations), although AI shows a slight statistical advantage.</p>
            
            <h2>4.4 Analysis of Execution Time</h2>
            
            <p>AI-generated tests run significantly faster than manual ones (AI median: 0.003s vs Manual: 0.012s, p = 3.74e-05). This 4x difference in execution speed has important practical implications in continuous integration and agile development cycles. The greater execution speed of AI tests can be attributed to their simpler structure and focus on main paths, versus the greater complexity of manual tests that include exhaustive validations.</p>
            
            <h2>4.5 Differentiated Qualitative Behavior</h2>
            
            <p>Beyond quantitative metrics, significant qualitative differences were observed:</p>
            
            <ul>
                <li><strong>Manual Tests:</strong> Reflect deep understanding of business domain, identification of edge scenarios, exceptional cases, and implicit behaviors. They require considerable time and cognitive investment, but produce greater functional exhaustiveness.</li>
                <li><strong>AI Tests:</strong> Provide rapid and systematic generation. ChatGPT shows strong dependence on prompt formulation. Diffblue Cover produces consistent tests based on bytecode analysis, with less user dependence but more superficial coverage of business logic.</li>
            </ul>
            
            <h2>4.6 Implications of Effect Size</h2>
            
            <p>Despite all Mann-Whitney U tests showing p &lt; 0.05 (statistical significance), effect sizes are small (r = 0.0575 to 0.1453). This means that although differences are real and detectable with N = 2,480, their practical magnitude is modest. In practical terms, both approaches (Manual and AI) are competitive in most dimensions, with small advantages varying by metric.</p>
            
            <h2>4.7 Synthesis: Complementary Strengths and Weaknesses</h2>
            
            <p><strong>Table 9:</strong> Complementary strengths and weaknesses of manual vs AI-generated tests.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Dimension</th>
                        <th>Manual Tests</th>
                        <th>AI Tests</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Instruction Coverage</strong></td>
                        <td>Superior (21.85% vs 10.69%)</td>
                        <td>Inferior</td>
                    </tr>
                    <tr>
                        <td><strong>Branch Coverage</strong></td>
                        <td>Inferior</td>
                        <td>Superior (16.25% vs 12.50%)</td>
                    </tr>
                    <tr>
                        <td><strong>Mutation Score</strong></td>
                        <td>Comparable</td>
                        <td>Slightly Superior</td>
                    </tr>
                    <tr>
                        <td><strong>Speed</strong></td>
                        <td>Slow (0.012s)</td>
                        <td>Fast (0.003s)</td>
                    </tr>
                    <tr>
                        <td><strong>Functional Understanding</strong></td>
                        <td>Deep and Contextual</td>
                        <td>Superficial, Bytecode-Based</td>
                    </tr>
                    <tr>
                        <td><strong>Scalability</strong></td>
                        <td>Requires manual effort</td>
                        <td>Highly Scalable</td>
                    </tr>
                </tbody>
            </table>
            
            <h1>5. STUDY LIMITATIONS</h1>
            
            <h2>5.1 Methodological Limitations</h2>
            
            <ul>
                <li><strong>Simple Analysis Unit:</strong> The study uses a single system under test (Spring PetClinic). Although it is a realistic application and represents a common testing use case, generalization to other systems requires additional validation.</li>
                <li><strong>Small Aggregated Sample (N=12):</strong> The aggregated level analysis is based on 12 test classes, which limits statistical power. Although the N=2,480 analysis compensates for this, conclusions about aggregated effectiveness should be considered cautiously.</li>
                <li><strong>Partial Context for AI:</strong> AI generators did not have access to the system's complete context (general architecture, documented design patterns), which could have limited their effectiveness.</li>
                <li><strong>Control of Confounding Variables:</strong> Time of day, system load, and execution variability could affect coverage and time metrics, although multiple executions were used to mitigate this.</li>
            </ul>
            
            <h2>5.2 Code Coverage Limitations</h2>
            
            <ul>
                <li><strong>Absence of Code Quality Metrics:</strong> The study does not evaluate cyclomatic complexity, maintainability, or readability of generated vs manual tests.</li>
                <li><strong>Limited Coverage Metrics:</strong> Instructions and branches are used, but path coverage, MC/DC coverage, or other more rigorous coverage criteria are not considered.</li>
                <li><strong>Absence of End-to-End Tests:</strong> The analysis focuses on unit tests. The effectiveness of both approaches in integration or E2E testing is not evaluated.</li>
            </ul>
            
            <h2>5.3 Validity Threats</h2>
            
            <h3>5.3.1 Internal Validity</h3>
            <p>Threats related to variable manipulation and measurement:</p>
            <ul>
                <li>Execution time fluctuations due to system resources</li>
                <li>Unhandled exceptions that could affect reported coverage</li>
                <li>Differences in analysis tool versions (JaCoCo, PITest)</li>
            </ul>
            
            <h3>5.3.2 External Validity</h3>
            <p>Threats related to generalization:</p>
            <ul>
                <li>Results specific to Spring PetClinic; generalization to other languages/frameworks uncertain</li>
                <li>Specific AI tools (ChatGPT, Diffblue); other generators might have different performances</li>
                <li>Variable human developers; experience affects quality of manual tests</li>
            </ul>
            
            <h3>5.3.3 Construct Validity</h3>
            <p>Threats related to concept measurement:</p>
            <ul>
                <li>Code coverage is not a perfect proxy for test quality</li>
                <li>Mutation score strongly depends on the set of generated mutants</li>
                <li>Absence of manual functional validation of correct behavior</li>
            </ul>
            
            <h3>5.3.4 Conclusion Validity</h3>
            <p>Threats related to statistical inferences:</p>
            <ul>
                <li>Differences in statistical power between N=12 and N=2,480 require careful interpretation</li>
                <li>Multiple comparisons could increase type I error rate (although Bonferroni not formally applied)</li>
                <li>Independence assumptions between observations could be violated (temporal correlation in executions)</li>
            </ul>
            
            <h1>6. FUTURE WORK</h1>
            
            <h2>6.1 Scope Extension</h2>
            
            <ul>
                <li><strong>Replication in Multiple Systems:</strong> Validate findings in additional Java applications (Struts, Hibernate, Spring Cloud), other languages (C#, Python), and emerging frameworks.</li>
                <li><strong>Evaluation of New AI Tools:</strong> Investigate performance of emerging tools like OpenAI's o1, Claude 3.5, Gemini 2, and agent-based systems.</li>
                <li><strong>Integration and E2E Tests:</strong> Extend analysis to multiple testing levels (integration, systems, acceptance).</li>
            </ul>
            
            <h2>6.2 Methodology Improvement</h2>
            
            <ul>
                <li><strong>Complementary Metrics:</strong> Incorporate code quality metrics, static analysis, test maintainability, and production defect metrics.</li>
                <li><strong>Automatic Feedback:</strong> Implement feedback loops where test results (coverage, detected mutants) are fed back to AI generators for iterative improvement.</li>
                <li><strong>In-Depth Qualitative Analysis:</strong> Conduct qualitative studies with developers on usability, comprehensibility, and confidence in generated tests.</li>
            </ul>
            
            <h2>6.3 Agent-Based Systems</h2>
            
            <ul>
                <li><strong>Orchestration of Multiple Models:</strong> Investigate systems where multiple specialized LLMs coordinate to generate tests (request router, coverage specialist, edge case specialist).</li>
                <li><strong>Agentic Feedback Loops:</strong> Agents that execute tests, analyze coverage, identify gaps, and automatically regenerate for improvement.</li>
                <li><strong>Integration with Analysis Tools:</strong> Agents that consult static analysis results, linter recommendations, and architecture suggestions.</li>
            </ul>
            
            <h1>7. GENERAL CONCLUSIONS</h1>
            
            <p>This study has provided a comprehensive and rigorous analysis of the comparative effectiveness of manual tests versus those generated by artificial intelligence in the context of a realistic complexity Java/Spring Boot application. The main findings reveal a nuanced landscape where both approaches have complementary strengths.</p>
            
            <h2>7.1 Findings Synthesis</h2>
            
            <ul>
                <li><strong>Complementarity of Approaches:</strong> Manual and AI tests exhibit strengths in different dimensions. While manual tests achieve superior instruction coverage (21.85% vs 10.69%), AI tests achieve higher branch coverage (16.25% vs 12.50%). Both approaches are statistically significant but with small effect sizes (r &lt; 0.15).</li>
                
                <li><strong>AI Operational Efficiency:</strong> The clearest advantage of AI-generated tests is their execution speed (4x faster: 0.003s vs 0.012s). In the context of continuous integration and agile development, this advantage has considerable practical importance.</li>
                
                <li><strong>Resolved Statistical Paradox:</strong> The absence of significant differences with N=12 versus their presence with N=2,480 has been explained and validated through statistical power analysis. The real effect exists but is small in magnitude.</li>
            </ul>
            
            <h2>7.2 Practical Recommendation: Hybrid Approach</h2>
            
            <p>Based on the empirical evidence, a <strong>hybrid and complementary approach</strong> is recommended:</p>
            
            <ul>
                <li><strong>Initial Phase (Rapid Coverage):</strong> Use AI generators to quickly obtain basic branch coverage and provide a fast regression suite.</li>
                <li><strong>Refinement Phase (Exhaustiveness):</strong> Complement with manual tests oriented towards edge cases, specific business scenarios, and deep functional validation.</li>
                <li><strong>Continuous Integration Phase:</strong> Take advantage of AI speed to run tests frequently while maintaining human quality control.</li>
            </ul>
            
            <h2>7.3 Implications for Future Research</h2>
            
            <p>This work opens multiple research directions: (1) evaluation of more sophisticated agentic systems that coordinate multiple LLMs, (2) research on automatic feedback loops where test results iteratively improve generation, and (3) extension to multi-language, multi-framework, and multi-level testing systems. The clear trend is towards incrementally sophisticated automation, with humans retaining control over critical architectural decisions and functional validation.</p>
            
        </div>
        
        <div class="footer">
            <p>Technical Report | Empirical Study Spring PetClinic | Period: May-December 2025</p>
            <p>Repository: https://github.com/solveighty/spring-petclinic-test.git</p>
        </div>
    </div>
</body>
</html>
